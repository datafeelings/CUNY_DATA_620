{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUNY MSDA DATA 620\n",
    "\n",
    "### Homework 12: Word sense disambiguation\n",
    "By Dmitriy Vecheruk  \n",
    "\n",
    "### Assignment  \n",
    "*The Senseval 2 Corpus contains data intended to train word-sense disambiguation classifiers. It contains data for four words: hard, interest, line, and serve. Choose one of these four words, and load the corresponding data. Using this dataset, build a classifier that predicts the correct sense tag for a given instance.* \n",
    "\n",
    "----\n",
    "### Solution\n",
    "\n",
    "From the four words available in the corpus, I have chosen the word **\"hard\"**. In order to build a word sense classifier, the following steps were taken:  \n",
    "  \n",
    "1) Inspect the dataset and clean if necessary, calculate the classifier accuracy baseline   \n",
    "2) Split the data into training and holdout (test) set  \n",
    "3) Extract the word context and part of speech features  \n",
    "4) Build a Naive Bayes classifier and test it using cross-validation  \n",
    "5) Test the classifier on the holdout set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Load, inspect and clean the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import string\n",
    "from nltk.corpus import senseval, stopwords\n",
    "from nltk.classify import accuracy, NaiveBayesClassifier, apply_features\n",
    "from random import seed,shuffle\n",
    "\n",
    "# nltk.download() # use NLTK Corpus downloader to get the senseval and stopwords corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "instances = senseval.instances('hard.pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4333"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(instances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, there are 4333 instances in the dataset, an individual instance represents a sentence with POS tags and an indicator of the position of the word \"hard\", as well as a label for the word sense used in the sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SensevalInstance(word=u'hard-a', position=33, context=[('producers', 'NNS'), ('of', 'IN'), ('action', 'NN'), ('shows', 'VBZ'), (',', ','), ('like', 'IN'), ('cannell', 'NNP'), (',', ','), ('are', 'VBP'), ('willing', 'JJ'), ('to', 'TO'), ('make', 'VB'), ('them', 'PRP'), ('at', 'IN'), ('a', 'DT'), ('bargain', 'NN'), ('price', 'NN'), ('to', 'TO'), ('help', 'VB'), ('cbs', 'NNP'), ('open', 'JJ'), ('up', 'IN'), ('a', 'DT'), ('new', 'JJ'), ('market', 'NN'), ('for', 'IN'), ('one-hour', 'JJ'), ('action', 'NN'), ('shows', 'VBZ'), (',', ','), ('which', 'WDT'), ('have', 'VBP'), ('become', 'VBN'), ('hard', 'JJ'), ('to', 'TO'), ('sell', 'VB'), ('in', 'IN'), ('the', 'DT'), ('rerun', 'NN'), ('market', 'NN'), ('.', '.')], senses=('HARD1',))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instances[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get senses per instance\n",
    "senses = [item.senses for item in instances]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('HARD1',): 3455, ('HARD2',): 502, ('HARD3',): 376})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(senses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the sense 'HARD1' is occurring vastly more often than the other two senses. Also, the `FreqDist` function has not counted any instances where the sense was not clear (multiple senses present in a single sentence), which means that all of the input instances can be used.  \n",
    "The **baseline classification accuracy** of the classifier is the share of the of the most wide-spread class in the data (in this case \"HARD1\"): 3455/4333 = **79.74%**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Split the data into training and holdout (test) set  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate training, dev-test, and holdout sets\n",
    "\n",
    "seed(42) # Inintialize random seed\n",
    "\n",
    "size = int(len(instances) * 0.1)\n",
    "train_set, holdout_set = instances[size:], instances[:size]\n",
    "\n",
    "def split_train_test(x, n_test):\n",
    "    \"\"\"Randomly splits a list into two lists with n_test records in one, \n",
    "    and the remainder in the other one.\"\"\"\n",
    "    \n",
    "    random.shuffle(x)\n",
    "    \n",
    "    return x[:n_test],x[n_test:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract the word context and part of speech features  \n",
    "In this part, I set up functions to extract words and part of speech (POS) tags around the target word from each instance. Then, the most frequent items from word and POS context per sense will be used as classifier features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the set of stopwords\n",
    "stop_words = list(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(l, ltypes=(list, tuple)):\n",
    "    \"\"\"Source: http://rightfootin.blogspot.de/2006/09/more-on-python-flatten.html\"\"\"\n",
    "    ltype = type(l)\n",
    "    l = list(l)\n",
    "    i = 0\n",
    "    while i < len(l):\n",
    "        while isinstance(l[i], ltypes):\n",
    "            if not l[i]:\n",
    "                l.pop(i)\n",
    "                i -= 1\n",
    "                break\n",
    "            else:\n",
    "                l[i:i + 1] = l[i]\n",
    "        i += 1\n",
    "    return ltype(l)\n",
    "\n",
    "def extract_before_after_features(instance,i=instance.position,k_before=4,k_after=4,remove_punct=True,stop_words=None, \n",
    "                                  min_word_length = None, return_lists = True):\n",
    "    \"\"\"Parses a SensevalInstance and returns a dictionary with k words and POS tags \n",
    "    before and k words and POS tags after the target word at position i applying the \n",
    "    filters: remove punctuation, remove stop words from a provided list, remove all \n",
    "    short words with a length under min_word_length\"\"\"\n",
    "\n",
    "    puncts = [item for item in string.punctuation] + [item*2 for item in string.punctuation]\n",
    "\n",
    "    # Extract before and after parts\n",
    "\n",
    "    sent_words = [item[0] for item in instance.context]\n",
    "    sent_pos = [item[1] for item in instance.context]\n",
    "    before = zip(sent_words[:i],sent_pos[:i])\n",
    "    after = zip(sent_words[i+1:],sent_pos[i+1:])\n",
    "\n",
    "    # Apply cleaning\n",
    "\n",
    "    if stop_words is not None:\n",
    "        before = [(word,pos) for (word,pos) in before if word not in stop_words]\n",
    "        after = [(word,pos) for (word,pos) in after if word not in stop_words]\n",
    "    if remove_punct is True:\n",
    "        before = [(word,pos) for (word,pos) in before if word not in puncts]\n",
    "        after = [(word,pos) for (word,pos) in after if word not in puncts]\n",
    "    if min_word_length is not None:\n",
    "        before = [(word,pos) for (word,pos) in before if len(word) > min_word_length]\n",
    "        after = [(word,pos) for (word,pos) in after if len(word) > min_word_length]\n",
    "        \n",
    "    output = dict(\n",
    "    words_before = [word for (word,pos) in before[-k_before:]],\n",
    "    pos_before = [pos for (word,pos) in before[-k_before:]],\n",
    "    words_after = [word for (word,pos) in after[:k_after]],\n",
    "    pos_after = [pos for (word,pos) in after[:k_after]]\n",
    "    )\n",
    "    \n",
    "#   Make sure that start/end of the sentence are tagged as empty \n",
    "    for k, v in output.iteritems():\n",
    "        if len(v) == 0:\n",
    "            output[k] = ['EMPTY']\n",
    "\n",
    "    if return_lists == True:\n",
    "        return output\n",
    "    \n",
    "    else:\n",
    "        feature_dict = dict()\n",
    "\n",
    "        for key in output.keys():\n",
    "            for idx, item in enumerate(output[key]):\n",
    "                feature_dict[key+'_'+str(idx)] = item\n",
    "        \n",
    "        return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SensevalInstance(word=u'hard-a', position=19, context=[('david', 'NNP'), ('ogden', 'NNP'), ('stiers', 'NNP'), ('makes', 'VBZ'), ('a', 'DT'), ('valiant', 'JJ'), ('effort', 'NN'), ('to', 'TO'), ('bring', 'VB'), ('the', 'DT'), ('town', 'NN'), (\"'s\", 'POS'), ('mayor', 'NN'), ('to', 'TO'), ('life', 'NN'), (',', ','), ('but', 'CC'), ('often', 'RB'), ('is', 'VBZ'), ('hard', 'JJ'), ('to', 'TO'), ('decipher', 'VB'), ('because', 'IN'), ('of', 'IN'), ('an', 'DT'), ('aggressive', 'JJ'), ('southern', 'NNP'), ('accent', 'NN'), ('and', 'CC'), ('blustery', 'JJ'), ('tone', 'NN'), ('.', '.')], senses=('HARD1',))\n"
     ]
    }
   ],
   "source": [
    "instance = train_set[0]\n",
    "print instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos_after': ['VB', 'JJ', 'NNP', 'NN', 'JJ'],\n",
       " 'pos_before': ['NN', 'POS', 'NN', 'NN', 'RB'],\n",
       " 'words_after': ['decipher', 'aggressive', 'southern', 'accent', 'blustery'],\n",
       " 'words_before': ['town', \"'s\", 'mayor', 'life', 'often']}"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_before_after_features(instance,i=instance.position,k_before=5,k_after=5,remove_punct=True,\n",
    "                              stop_words=stop_words,min_word_length=None,return_lists = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can inspect the most frequent contexts per sense to understand the differences between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_features(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sense: HARD1\n",
      "\n",
      "Most common words before:,\n",
      "[('EMPTY', 714), ('would', 119), ('said', 112), ('make', 92), ('even', 65), ('much', 63), ('may', 59), ('going', 59), ('find', 59), ('makes', 47), ('people', 46), ('really', 46), ('one', 44), ('made', 36), ('like', 36)] \n",
      "\n",
      "Most common words after:,\n",
      "[('said', 238), ('time', 232), ('get', 144), ('believe', 130), ('find', 117), ('imagine', 92), ('EMPTY', 84), ('say', 83), ('see', 78), ('people', 68), ('way', 62), ('tell', 59), ('part', 58), ('one', 57), ('come', 57)] \n",
      "\n",
      "Sense: HARD2\n",
      "\n",
      "Most common words before:,\n",
      "[('take', 48), ('EMPTY', 44), ('said', 24), ('long', 17), ('taking', 15), ('years', 12), ('little', 12), ('get', 11), ('one', 10), ('good', 9), ('people', 9), (\"'re\", 9), ('took', 8), ('lot', 8), ('also', 8)] \n",
      "\n",
      "Most common words after:,\n",
      "[('work', 156), ('look', 81), ('feelings', 32), ('said', 22), ('line', 17), ('time', 14), ('EMPTY', 11), ('way', 10), ('day', 8), ('people', 8), ('business', 7), ('freedom', 7), ('fast', 7), ('evidence', 7), ('says', 6)] \n",
      "\n",
      "Sense: HARD3\n",
      "\n",
      "Most common words before:,\n",
      "[('rock', 27), ('EMPTY', 13), ('pkd', 10), ('made', 10), ('one', 8), ('said', 8), ('time', 8), ('caught', 7), ('ground', 7), ('firm', 7), ('good', 6), ('first', 6), ('like', 6), ('may', 5), ('including', 5)] \n",
      "\n",
      "Most common words after:,\n",
      "[('place', 22), ('cover', 17), ('surface', 15), ('EMPTY', 14), ('red', 12), ('soft', 12), ('plastic', 11), ('wheat', 9), ('material', 8), ('packed', 8), ('surfaces', 8), ('cheese', 7), ('shell', 7), ('edge', 7), ('spring', 7)] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "senses = ['HARD1','HARD2','HARD3']\n",
    "\n",
    "for sense in senses:\n",
    "    sense_instances = [item for item in train_set if item.senses[0] == sense]\n",
    "\n",
    "    before = []\n",
    "    after = []\n",
    "    for instance in sense_instances:\n",
    "        features = extract_before_after_features(instance,i=instance.position,k_before=5,k_after=5,\n",
    "                                                 remove_punct=True,stop_words=stop_words,min_word_length=2,return_lists = True)\n",
    "        before.append(features['words_before'])\n",
    "        after.append(features['words_after'])\n",
    "    \n",
    "    before = flatten(before)\n",
    "    after = flatten(after)\n",
    "    print 'Sense: '+sense+'\\n'\n",
    "    print 'Most common words before:,\\n',nltk.FreqDist(before).most_common(15),'\\n'\n",
    "    print 'Most common words after:,\\n',nltk.FreqDist(after).most_common(15),'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the distributions above that `HARD3` obviously stands for a physical property of withstanding pressure, but the difference between `HARD1` and `HARD2` is more subtle. It seems as if the first sense is more related to \"difficulty\" (hard time, hard to get, hard to believe), whereas the second is more about \"a lack of kindness\" (hard look, hard feelings, hard line)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build a Naive Bayes classifier and test it using cross-validation  \n",
    "In this part, the most frequent word and POS features per sense will be used to train a Naive Bayes classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3120 780\n"
     ]
    }
   ],
   "source": [
    "# Split into training and test\n",
    "size = int(len(train_set) * 0.2)\n",
    "training, dev_test = train_set[size:], train_set[:size]\n",
    "print len(training), len(dev_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "# training_data = []\n",
    "\n",
    "# for instance in training:\n",
    "#     features = extract_before_after_features(instance,i=instance.position,k_before=3,k_after=3,remove_punct=True,\n",
    "#                                              stop_words=stop_words,min_word_length=2, )\n",
    "#     label = instance.senses[0]\n",
    "#     training_data.append( (features,label) )\n",
    "\n",
    "# training_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "# A list of classified featuresets, i.e., a list of tuples ``(featureset, label)``.\n",
    " \n",
    "# label = instance.senses[0]\n",
    "    \n",
    "def featurizer_1(instance):\n",
    "    return (extract_before_after_features(instance,i=instance.position,k_before=1,k_after=1,\n",
    "                                                     remove_punct=True,stop_words=None,min_word_length=2,\n",
    "                                                     return_lists = False) )\n",
    "\n",
    "def labeler(instance):\n",
    "    return instance.senses[0]\n",
    "\n",
    "training_ft = [ (featurizer_1(instance), labeler(instance)) for instance in training]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'pos_after_0': 'VB',\n",
       "  'pos_before_0': 'NN',\n",
       "  'words_after_0': 'describe',\n",
       "  'words_before_0': 'kind'},\n",
       " 'HARD1')"
      ]
     },
     "execution_count": 503,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_ft[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_0 = nltk.NaiveBayesClassifier.train(training_ft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "             pos_after_0 = 'VB'            HARD1 : HARD2  =    183.3 : 1.0\n",
      "           words_after_0 = 'look'          HARD2 : HARD1  =     89.5 : 1.0\n",
      "           words_after_0 = 'work'          HARD2 : HARD3  =     88.1 : 1.0\n",
      "           words_after_0 = 'for'           HARD1 : HARD2  =     41.6 : 1.0\n",
      "             pos_after_0 = 'VBN'           HARD3 : HARD1  =     27.4 : 1.0\n",
      "          words_before_0 = 'EMPTY'         HARD1 : HARD3  =     24.5 : 1.0\n",
      "           words_after_0 = 'cover'         HARD3 : HARD1  =     23.8 : 1.0\n",
      "             pos_after_0 = 'IN'            HARD1 : HARD2  =     22.0 : 1.0\n",
      "          words_before_0 = 'long'          HARD2 : HARD1  =     20.8 : 1.0\n",
      "           words_after_0 = 'believe'       HARD1 : HARD2  =     19.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier_0.show_most_informative_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITE INTERPRETATION HERE about the most informative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dev_test, featurizer, classifier):\n",
    "    \"\"\"\n",
    "    Returns the accuracy, a contingency table, and the most informative features\n",
    "    of an NLTK classifier.\n",
    "    Based on the code from: Natural Language Processing with Python\n",
    "    by Steven Bird, Ewan Klein, and Edward Loper. 2009, O'Reilly Media\n",
    "    \"\"\"\n",
    "    # Generate test features\n",
    "    dev_test_ft = [ (featurizer(instance), labeler(instance)) for instance in dev_test]\n",
    "\n",
    "    # Score the model on the test features\n",
    "    model_out = [classifier.classify(item[0]) for item in dev_test_ft]\n",
    "\n",
    "    # Evaluate the accuracy\n",
    "    accr = nltk.classify.accuracy(classifier, dev_test_ft)\n",
    "    \n",
    "\n",
    "    # Confusion matrix\n",
    "    true_label = [item[1] for item in dev_test_ft]\n",
    "    \n",
    "    \n",
    "    # Errors: Compare the true label with the classifier output\n",
    "    errors = []\n",
    "    for item in zip(dev_test_ft,model_out):\n",
    "        if item[0][1] != item[1]:\n",
    "            errors.append( (item[0][1], item[1], item[0][0]))\n",
    "    \n",
    "    return dict(model_out = model_out,true_label=true_label,accr=accr,errors=errors)\n",
    "\n",
    "def print_eval_results(classifier, eval_output,err_cnt):\n",
    "    \"Prints model evaluation output\"\n",
    "    \n",
    "    \n",
    "    print (\"Accuracy on the test set: {}% \\n\".format(eval_output[\"accr\"]*100))\n",
    "    # Main features \n",
    "    classifier.show_most_informative_features(10)\n",
    "    \n",
    "    print \"\\n Confusion Matrix: \\n\"\n",
    "    print nltk.ConfusionMatrix(eval_output[\"true_label\"],eval_output[\"model_out\"])\n",
    "    print \"\\n Errors: \\n\", \n",
    "    for (tag, guess, name) in eval_output[\"errors\"][:err_cnt]: \n",
    "        print 'correct=%-8s guess=%-8s name=%-30s' %(tag, guess, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the test set: 92.9487179487% \n",
      "\n",
      "Most Informative Features\n",
      "             pos_after_0 = 'VB'            HARD1 : HARD2  =    183.3 : 1.0\n",
      "           words_after_0 = 'look'          HARD2 : HARD1  =     89.5 : 1.0\n",
      "           words_after_0 = 'work'          HARD2 : HARD3  =     88.1 : 1.0\n",
      "           words_after_0 = 'for'           HARD1 : HARD2  =     41.6 : 1.0\n",
      "             pos_after_0 = 'VBN'           HARD3 : HARD1  =     27.4 : 1.0\n",
      "          words_before_0 = 'EMPTY'         HARD1 : HARD3  =     24.5 : 1.0\n",
      "           words_after_0 = 'cover'         HARD3 : HARD1  =     23.8 : 1.0\n",
      "             pos_after_0 = 'IN'            HARD1 : HARD2  =     22.0 : 1.0\n",
      "          words_before_0 = 'long'          HARD2 : HARD1  =     20.8 : 1.0\n",
      "           words_after_0 = 'believe'       HARD1 : HARD2  =     19.7 : 1.0\n",
      "\n",
      " Confusion Matrix: \n",
      "\n",
      "      |   H   H   H |\n",
      "      |   A   A   A |\n",
      "      |   R   R   R |\n",
      "      |   D   D   D |\n",
      "      |   1   2   3 |\n",
      "------+-------------+\n",
      "HARD1 |<725> 42  13 |\n",
      "HARD2 |   .  <.>  . |\n",
      "HARD3 |   .   .  <.>|\n",
      "------+-------------+\n",
      "(row = reference; col = test)\n",
      "\n",
      "\n",
      " Errors: \n",
      "correct=HARD1    guess=HARD2    name={'words_after_0': 'way', 'pos_before_0': 'DT', 'pos_after_0': 'NN', 'words_before_0': 'the'}\n",
      "correct=HARD1    guess=HARD3    name={'words_after_0': 'but', 'pos_before_0': 'VBZ', 'pos_after_0': 'CC', 'words_before_0': 'looks'}\n",
      "correct=HARD1    guess=HARD2    name={'words_after_0': 'school', 'pos_before_0': 'DT', 'pos_after_0': 'NN', 'words_before_0': 'the'}\n",
      "correct=HARD1    guess=HARD2    name={'words_after_0': 'way', 'pos_before_0': 'DT', 'pos_after_0': 'NN', 'words_before_0': 'the'}\n",
      "correct=HARD1    guess=HARD3    name={'words_after_0': 'cold', 'pos_before_0': 'VBZ', 'pos_after_0': 'JJ', 'words_before_0': 'causes'}\n",
      "correct=HARD1    guess=HARD2    name={'words_after_0': 'way', 'pos_before_0': 'DT', 'pos_after_0': 'NN', 'words_before_0': 'the'}\n",
      "correct=HARD1    guess=HARD2    name={'words_after_0': 'economic', 'pos_before_0': 'IN', 'pos_after_0': 'JJ', 'words_before_0': 'off'}\n",
      "correct=HARD1    guess=HARD2    name={'words_after_0': 'way', 'pos_before_0': 'DT', 'pos_after_0': 'NN', 'words_before_0': 'the'}\n",
      "correct=HARD1    guess=HARD2    name={'words_after_0': 'struggle', 'pos_before_0': 'JJ', 'pos_after_0': 'NN', 'words_before_0': 'long'}\n",
      "correct=HARD1    guess=HARD3    name={'words_after_0': 'EMPTY', 'pos_before_0': 'NN', 'pos_after_0': 'EMPTY', 'words_before_0': 'atmosphere'}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the features and classifier\n",
    "eval_0 = evaluate_model(dev_test=dev_test, featurizer=featurizer_1,classifier=classifier_0)\n",
    "print_eval_results(classifier_0,eval_0,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pos_after_0': 'VB',\n",
       " 'pos_after_1': 'JJ',\n",
       " 'pos_after_2': 'NNP',\n",
       " 'pos_after_3': 'NN',\n",
       " 'pos_before_0': 'NN',\n",
       " 'pos_before_1': 'NN',\n",
       " 'pos_before_2': 'NN',\n",
       " 'pos_before_3': 'RB',\n",
       " 'words_after_0': 'decipher',\n",
       " 'words_after_1': 'aggressive',\n",
       " 'words_after_2': 'southern',\n",
       " 'words_after_3': 'accent',\n",
       " 'words_before_0': 'town',\n",
       " 'words_before_1': 'mayor',\n",
       " 'words_before_2': 'life',\n",
       " 'words_before_3': 'often'}"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract features\n",
    "dev_test_data = []\n",
    "\n",
    "for instance in dev_test:\n",
    "    \n",
    "#     label = instance.senses[0]\n",
    "\n",
    "    features_raw = extract_before_after_features(instance,i=instance.position,k_before=4,k_after=4,\n",
    "                                                     remove_punct=True,stop_words=stop_words,min_word_length=2)\n",
    "\n",
    "    feature_dict = dict()\n",
    "\n",
    "    for key in features_raw.keys():\n",
    "        for idx, item in enumerate(features_raw[key]):\n",
    "            feature_dict[key+'_'+str(idx)] = item\n",
    "\n",
    "    dev_test_data.append( (feature_dict) )\n",
    "\n",
    "dev_test_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HARD1'"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_0.classify(dev_test_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(dev_test, featurizer, classifier):\n",
    "    \"\"\"\n",
    "    Returns the accuracy, a contingency table, and the most informative features\n",
    "    of an NLTK classifier.\n",
    "    Based on the code from: Natural Language Processing with Python\n",
    "    by Steven Bird, Ewan Klein, and Edward Loper. 2009, O'Reilly Media\n",
    "    \"\"\"\n",
    "    # Generate test features\n",
    "    dev_test_ft = apply_features(featurizer, dev_test)\n",
    "\n",
    "    # Score the model on the test features\n",
    "    model_out = [classifier.classify(item[0]) for item in dev_test_ft]\n",
    "\n",
    "    # Evaluate the accuracy\n",
    "    accr = nltk.classify.accuracy(classifier, dev_test_ft)\n",
    "    \n",
    "\n",
    "    # Confusion matrix\n",
    "    true_label = [item[1] for item in dev_test_ft]\n",
    "    \n",
    "    \n",
    "    # Errors: Compare the true label with the classifier output\n",
    "    errors = []\n",
    "    for item in zip(dev_test,model_out):\n",
    "        if item[0][1] != item[1]:\n",
    "            errors.append( (item[0][1], item[1], item[0][0]))\n",
    "    \n",
    "    return dict(model_out = model_out,true_label=true_label,accr=accr,errors=errors)\n",
    "        \n",
    "def print_eval_results(classifier, eval_output,err_cnt):\n",
    "    \"Prints model evaluation output\"\n",
    "    \n",
    "    \n",
    "    print (\"Accuracy on the test set: {}% \\n\".format(eval_output[\"accr\"]*100))\n",
    "    # Main features \n",
    "    classifier.show_most_informative_features(10)\n",
    "    \n",
    "    print \"\\n Confusion Matrix: \\n\"\n",
    "    print nltk.ConfusionMatrix(eval_output[\"true_label\"],eval_output[\"model_out\"])\n",
    "    print \"\\n Errors: \\n\", \n",
    "    for (tag, guess, name) in eval_output[\"errors\"][:err_cnt]: \n",
    "        print 'correct=%-8s guess=%-8s name=%-30s' %(tag, guess, name)\n",
    "\n",
    "def iterate_nb_validation(data, test_size, featurizer,n_iter):\n",
    "    \n",
    "    accuracy_out = []\n",
    "    \n",
    "    for item in range(0,n_iter):\n",
    "    \n",
    "        # Split dataset\n",
    "        dev_test, train_set = split_train_test(data, test_size) \n",
    "\n",
    "        # Generate training features\n",
    "        train_ft = apply_features(featurizer, train_set)\n",
    "    \n",
    "        # Apply Naive Bayes classifier\n",
    "        classifier = nltk.NaiveBayesClassifier.train(train_ft)\n",
    "        eval_result = evaluate_model(dev_test=dev_test, featurizer=featurizer,classifier=classifier)\n",
    "        \n",
    "        accuracy_out.append(eval_result[\"accr\"])\n",
    "        print \".\",\n",
    "    \n",
    "    return accuracy_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "    print 'Training classifier...'\n",
    "    classifier = trainer([(features(i, vocab, distance), label) for (i, label) in training_data])\n",
    "    # Test classifier\n",
    "    print 'Testing classifier...'\n",
    "    acc = accuracy(classifier, [(features(i, vocab, distance), label) for (i, label) in test_data] )\n",
    "    print 'Accuracy: %6.4f' % acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "dev_test, train_set = split_train_test(training_data, 500) \n",
    "\n",
    "# Generate training features\n",
    "train_ft = apply_features(gender_features_0, train_set)\n",
    "\n",
    "# Apply Naive Bayes classifier\n",
    "classifier_0 = nltk.NaiveBayesClassifier.train(train_ft)\n",
    "\n",
    "# Evaluate the features and classifier\n",
    "eval_0 = evaluate_model(dev_test=dev_test, featurizer=gender_features_0,classifier=classifier_0)\n",
    "print_eval_results(classifier_0,eval_0,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference \n",
    "1) [Chapter 6: Learning to Classify Text](http://www.nltk.org/book/ch06.html) from Natural Language Processing with Python, by Steven Bird, Ewan Klein and Edward Loper, Copyright © 2014 the authors   \n",
    "2) University of Edinburgh [FNLP 2017: Lab Session 5: Word Sense Disambiguation](https://www.inf.ed.ac.uk/teaching/courses/fnlp/Tutorials/7_WSD/tutorial.html)\n",
    "Henry S. Thompson, based on original by Alex Lascarides, 2017  \n",
    "3) https://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python  \n",
    "4) https://docs.python.org/2/library/stdtypes.html#dict.iteritems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
